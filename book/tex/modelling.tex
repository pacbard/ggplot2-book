\hypertarget{cha:modelling}{%
\chapter{Modelling for visualisation}\label{cha:modelling}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Modelling is an essential tool for visualisation. There are two
particularly strong connections between modelling and visualisation that
I want to explore in this chapter: \index{Modelling}

\begin{itemize}
\item
  Using models as a tool to remove obvious patterns in your plots. This
  is useful because strong patterns mask subtler effects. Often the
  strongest effects are already known and expected, and removing them
  allows you to see surprises more easily.
\item
  Other times you have a lot of data, too much to show on a handful of
  plots. Models can be a powerful tool for summarising data so that you
  get a higher level view.
\end{itemize}

In this chapter, I'm going to focus on the use of linear models to
acheive these goals. Linear models are a basic, but powerful, tool of
statistics, and I recommend that everyone serious about visualisation
learns at least the basics of how to use them. To this end, I highly
recommend two books by Julian J. Faraway:

\begin{itemize}
\tightlist
\item
  Linear Models with R \url{http://amzn.com/1439887330}
\item
  Extending the Linear Model with R \url{http://amzn.com/158488424X}
\end{itemize}

These books cover some of the theory of linear models, but are pragmatic
and focussed on how to actually use linear models (and their extensions)
in R. \index{Linear models}

There are many other modelling tools, which I don't have the space to
show. If you understand how linear models can help improve your
visualisations, you should be able to translate the basic idea to other
families of models. This chapter just scratches the surface of what you
can do. But hopefully it reinforces how visualisation can combine with
modelling to help you build a powerful data analysis toolbox. For more
ideas, check out Wickham, Cook, and Hofmann (2015).

This chapter only scratches the surface of the intersection between
visualisation and modelling. In my opinion, mastering the combination of
visualisations and models is key to being an effective data scientist.
Unfortunately most books (like this one!) only focus on either
visualisation or modelling, but not both. There's a lot of interesting
work to be done.

\hypertarget{sub:trend}{%
\section{Removing trend}\label{sub:trend}}

So far our analysis of the diamonds data has been plagued by the
powerful relationship between size and price. It makes it very difficult
to see the impact of cut, colour and clarity because higher quality
diamonds tend to be smaller, and hence cheaper. This challenge is often
called confounding. We can use a linear model to remove the effect of
size on price. Instead of looking at the raw price, we can look at the
relative price: how valuable is this diamond relative to the average
diamond of the same size. \index{Removing trend}

To get started, we'll focus on diamonds of size two carats or less (96\%
of the dataset). This avoids some incidental problems that you can
explore in the exercises if you're interested. We'll also create two new
variables: log price and log carat. These variables are useful because
they produce a plot with a strong linear trend.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(carat }\OperatorTok{<=}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{lcarat =} \KeywordTok{log2}\NormalTok{(carat),}
    \DataTypeTok{lprice =} \KeywordTok{log2}\NormalTok{(price)}
\NormalTok{  )}
\NormalTok{diamonds2}
\CommentTok{#> # A tibble: 52,051 x 12}
\CommentTok{#>   carat cut       color clari~ depth table price     x     y     z}
\CommentTok{#>   <dbl> <ord>     <ord> <ord>  <dbl> <dbl> <int> <dbl> <dbl> <dbl>}
\CommentTok{#> 1 0.230 Ideal     E     SI2     61.5  55.0   326  3.95  3.98  2.43}
\CommentTok{#> 2 0.210 Premium   E     SI1     59.8  61.0   326  3.89  3.84  2.31}
\CommentTok{#> 3 0.230 Good      E     VS1     56.9  65.0   327  4.05  4.07  2.31}
\CommentTok{#> 4 0.290 Premium   I     VS2     62.4  58.0   334  4.20  4.23  2.63}
\CommentTok{#> 5 0.310 Good      J     SI2     63.3  58.0   335  4.34  4.35  2.75}
\CommentTok{#> 6 0.240 Very Good J     VVS2    62.8  57.0   336  3.94  3.96  2.48}
\CommentTok{#> # ... with 5.204e+04 more rows, and 2 more variables: lcarat}
\CommentTok{#> #   <dbl>, lprice <dbl>}

\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(lcarat, lprice)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bin2d}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"yellow"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{_figures/modelling/unnamed-chunk-1-1}
\end{figure}

In the graphic we used \texttt{geom\_smooth()} to overlay the line of
best fit to the data. We can replicate this outside of ggplot2 by
fitting a linear model with \texttt{lm()}. This allows us to find out
the slope and intercept of the line: \indexf{lm} \indexf{coef}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(lprice }\OperatorTok{~}\StringTok{ }\NormalTok{lcarat, }\DataTypeTok{data =}\NormalTok{ diamonds2)}
\KeywordTok{coef}\NormalTok{(}\KeywordTok{summary}\NormalTok{(mod))}
\CommentTok{#>             Estimate Std. Error t value Pr(>|t|)}
\CommentTok{#> (Intercept)     12.2    0.00211    5789        0}
\CommentTok{#> lcarat           1.7    0.00208     816        0}
\end{Highlighting}
\end{Shaded}

If you're familiar with linear models, you might want to interpret those
coefficients: \(\log_2(price) = 12.2 + 1.7 \cdot \log_2(carat)\), which
implies \(price = 4900 \cdot carat ^ {1.7}\). Interpreting those
coefficients certainly is useful, but even if you don't understand them,
the model can still be useful. We can use it to subtract the trend away
by looking at the residuals: the price of each diamond minus its
predicted price, based on weight alone. Geometrically, the residuals are
the vertical distance between each point and the line of best fit. They
tell us the price relative to the ``average'' diamond of that size.
\indexf{resid}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds2 <-}\StringTok{ }\NormalTok{diamonds2 }\OperatorTok{%>%}\StringTok{ }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rel_price =} \KeywordTok{resid}\NormalTok{(mod))}
\KeywordTok{ggplot}\NormalTok{(diamonds2, }\KeywordTok{aes}\NormalTok{(carat, rel_price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_bin2d}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{_figures/modelling/unnamed-chunk-3-1}
\end{figure}

A relative price of zero means that the diamond was at the average
price; positive means that it's more expensive than expected (based on
its size), and negative means that it's cheaper than expected.

Interpreting the values precisely is a little tricky here because we've
log-transformed price. The residuals give the absolute difference
(\(x - expected\)), but here we have
\(\log_2(price) - \log_2(expected price)\), or equivalently
\(\log_2(price / expected price)\). If we ``back-transform'' to the
original scale by applying the opposite transformation (\(2 ^ x\)) we
get \(price / expected price\). This makes the values more
interpretable, at the cost of the nice symmetry property of the logged
values (i.e.~both relatively cheaper and relatively more expensive
diamonds have the same range). We can make a little table to help
interpret the values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xgrid <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{by =} \DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{logx =}\NormalTok{ xgrid, }\DataTypeTok{x =} \KeywordTok{round}\NormalTok{(}\DecValTok{2} \OperatorTok{^}\StringTok{ }\NormalTok{xgrid, }\DecValTok{2}\NormalTok{))}
\CommentTok{#>      logx    x}
\CommentTok{#> 1  -2.000 0.25}
\CommentTok{#> 2  -1.667 0.31}
\CommentTok{#> 3  -1.333 0.40}
\CommentTok{#> 4  -1.000 0.50}
\CommentTok{#> 5  -0.667 0.63}
\CommentTok{#> 6  -0.333 0.79}
\CommentTok{#> 7   0.000 1.00}
\CommentTok{#> 8   0.333 1.26}
\CommentTok{#> 9   0.667 1.59}
\CommentTok{#> 10  1.000 2.00}
\end{Highlighting}
\end{Shaded}

This table illustrates why we used \texttt{log2()} rather than
\texttt{log()}: a change of 1 unit on the logged scale, corresponding to
a doubling on the original scale. For example, a \texttt{rel\_price} of
-1 means that it's half of the expected price; a relative price of 1
means that it's twice the expected price. \index{Log!transform}

Let's use both price and relative price to see how colour and cut affect
the value of a diamond. We'll compute the average price and average
relative price for each combination of colour and cut:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{color_cut <-}\StringTok{ }\NormalTok{diamonds2 }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(color, cut) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{price =} \KeywordTok{mean}\NormalTok{(price), }
    \DataTypeTok{rel_price =} \KeywordTok{mean}\NormalTok{(rel_price)}
\NormalTok{  )}
\NormalTok{color_cut}
\CommentTok{#> # A tibble: 35 x 4}
\CommentTok{#> # Groups: color [?]}
\CommentTok{#>   color cut       price rel_price}
\CommentTok{#>   <ord> <ord>     <dbl>     <dbl>}
\CommentTok{#> 1 D     Fair       3939   -0.0755}
\CommentTok{#> 2 D     Good       3309   -0.0472}
\CommentTok{#> 3 D     Very Good  3368    0.104 }
\CommentTok{#> 4 D     Premium    3513    0.109 }
\CommentTok{#> 5 D     Ideal      2595    0.217 }
\CommentTok{#> 6 E     Fair       3516   -0.172 }
\CommentTok{#> # ... with 29 more rows}
\end{Highlighting}
\end{Shaded}

If we look at price, it's hard to see how the quality of the diamond
affects the price. The lowest quality diamonds (fair cut with colour J)
have the highest average value! This is because those diamonds also tend
to be larger: size and quality are confounded.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(color_cut, }\KeywordTok{aes}\NormalTok{(color, price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ cut), }\DataTypeTok{colour =} \StringTok{"grey80"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =}\NormalTok{ cut))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{_figures/modelling/unnamed-chunk-6-1}
\end{figure}

If however, we plot the relative price, you see the pattern that you
expect: as the quality of the diamonds decreases, the relative price
decreases. The worst quality diamond is 0.61x (\(2 ^ {-0.7}\)) the price
of an ``average'' diamond.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(color_cut, }\KeywordTok{aes}\NormalTok{(color, rel_price)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ cut), }\DataTypeTok{colour =} \StringTok{"grey80"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{colour =}\NormalTok{ cut))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\linewidth]{_figures/modelling/unnamed-chunk-7-1}
\end{figure}

This technique can be employed in a wide range of situations. Wherever
you can explicitly model a strong pattern that you see in a plot, it's
worthwhile to use a model to remove that strong pattern so that you can
see what interesting trends remain.

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What happens if you repeat the above analysis with all diamonds? (Not
  just all diamonds with two or fewer carats). What does the strange
  geometry of \texttt{log(carat)} vs relative price represent? What does
  the diagonal line without any points represent?
\item
  I made an unsupported assertion that lower-quality diamonds tend to be
  larger. Support my claim with a plot.
\item
  Can you create a plot that simultaneously shows the effect of colour,
  cut, and clarity on relative price? If there's too much information to
  show on one plot, think about how you might create a sequence of plots
  to convey the same message.
\item
  How do depth and table relate to the relative price?
\end{enumerate}

\hypertarget{texas-housing-data}{%
\section{Texas housing data}\label{texas-housing-data}}

We'll continue to explore the connection between modelling and
visualisation with the \texttt{txhousing} dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{txhousing}
\CommentTok{#> # A tibble: 8,034 x 9}
\CommentTok{#>   city     year month sales   volume median listings invent~  date}
\CommentTok{#>   <chr>   <int> <int> <dbl>    <dbl>  <dbl>    <dbl>   <dbl> <dbl>}
\CommentTok{#> 1 Abilene  2000     1  72.0  5380000  71400      701    6.30  2000}
\CommentTok{#> 2 Abilene  2000     2  98.0  6505000  58700      746    6.60  2000}
\CommentTok{#> 3 Abilene  2000     3 130    9285000  58100      784    6.80  2000}
\CommentTok{#> 4 Abilene  2000     4  98.0  9730000  68600      785    6.90  2000}
\CommentTok{#> 5 Abilene  2000     5 141   10590000  67300      794    6.80  2000}
\CommentTok{#> 6 Abilene  2000     6 156   13910000  66900      780    6.60  2000}
\CommentTok{#> # ... with 8,028 more rows}
\end{Highlighting}
\end{Shaded}

This data was collected by the Real Estate Center at Texas A\&M
University, \url{http://recenter.tamu.edu/Data/hs/}. The data contains
information about 46 Texas cities, recording the number of house sales
(\texttt{sales}), the total volume of sales (\texttt{volume}), the
\texttt{average} and \texttt{median} sale prices, the number of houses
listed for sale (\texttt{listings}) and the number of months inventory
(\texttt{inventory}). Data is recorded monthly from Jan 2000 to Apr
2015, 187 entries for each city.
\index{Data!txhousing@\texttt{txhousing}}

We're going to explore how sales have varied over time for each city as
it shows some interesting trends and poses some interesting challenges.
Let's start with an overview: a time series of sales for each city:
\index{Data!longitudinal}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(txhousing, }\KeywordTok{aes}\NormalTok{(date, sales)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ city), }\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \includegraphics[width=1\linewidth]{_figures/modelling/unnamed-chunk-9-1}
\end{figure}

Two factors make it hard to see the long-term trend in this plot:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The range of sales varies over multiple orders of magnitude. The
  biggest city, Houston, averages over \textasciitilde{}4000 sales per
  month; the smallest city, San Marcos, only averages
  \textasciitilde{}20 sales per month.
\item
  There is a strong seasonal trend: sales are much higher in the summer
  than in the winter.
\end{enumerate}

We can fix the first problem by plotting log sales:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(txhousing, }\KeywordTok{aes}\NormalTok{(date, }\KeywordTok{log}\NormalTok{(sales))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ city), }\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \includegraphics[width=1\linewidth]{_figures/modelling/unnamed-chunk-10-1}
\end{figure}

We can fix the second problem using the same technique we used for
removing the trend in the diamonds data: we'll fit a linear model and
look at the residuals. This time we'll use a categorical predictor to
remove the month effect. First we check that the technique works by
applying it to a single city. It's always a good idea to start simple so
that if something goes wrong you can more easily pinpoint the problem.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{abilene <-}\StringTok{ }\NormalTok{txhousing }\OperatorTok{%>%}\StringTok{ }\KeywordTok{filter}\NormalTok{(city }\OperatorTok{==}\StringTok{ "Abilene"}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(abilene, }\KeywordTok{aes}\NormalTok{(date, }\KeywordTok{log}\NormalTok{(sales))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}

\NormalTok{mod <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\KeywordTok{log}\NormalTok{(sales) }\OperatorTok{~}\StringTok{ }\KeywordTok{factor}\NormalTok{(month), }\DataTypeTok{data =}\NormalTok{ abilene)}
\NormalTok{abilene}\OperatorTok{$}\NormalTok{rel_sales <-}\StringTok{ }\KeywordTok{resid}\NormalTok{(mod)}
\KeywordTok{ggplot}\NormalTok{(abilene, }\KeywordTok{aes}\NormalTok{(date, rel_sales)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \includegraphics[width=0.5\linewidth]{_figures/modelling/unnamed-chunk-11-1}%
  \includegraphics[width=0.5\linewidth]{_figures/modelling/unnamed-chunk-11-2}
\end{figure}

We can apply this transformation to every city with \texttt{group\_by()}
and \texttt{mutate()}. Note the use of \texttt{na.action\ =\ na.exclude}
argument to \texttt{lm()}. Counterintuitively this ensures that missing
values in the input are matched with missing values in the output
predictions and residuals. Without this argument, missing values are
just dropped, and the residuals don't line up with the inputs.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{deseas <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, month) \{}
  \KeywordTok{resid}\NormalTok{(}\KeywordTok{lm}\NormalTok{(x }\OperatorTok{~}\StringTok{ }\KeywordTok{factor}\NormalTok{(month), }\DataTypeTok{na.action =}\NormalTok{ na.exclude))}
\NormalTok{\}}

\NormalTok{txhousing <-}\StringTok{ }\NormalTok{txhousing }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(city) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{rel_sales =} \KeywordTok{deseas}\NormalTok{(}\KeywordTok{log}\NormalTok{(sales), month))}
\end{Highlighting}
\end{Shaded}

With this data in hand, we can re-plot the data. Now that we have
log-transformed the data and removed the strong seasonal effects we can
see there is a strong common pattern: a consistent increase from
2000-2007, a drop until 2010 (with quite some noise), and then a gradual
rebound. To make that more clear, I included a summary line that shows
the mean relative sales across all cities.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(txhousing, }\KeywordTok{aes}\NormalTok{(date, rel_sales)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ city), }\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{5}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"summary"}\NormalTok{, }\DataTypeTok{fun.y =} \StringTok{"mean"}\NormalTok{, }\DataTypeTok{colour =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \includegraphics[width=1\linewidth]{_figures/modelling/unnamed-chunk-13-1}
\end{figure}

(Note that removing the seasonal effect also removed the intercept - we
see the trend for each city relative to its average number of sales.)

\hypertarget{exercises-1}{%
\subsection{Exercises}\label{exercises-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The final plot shows a lot of short-term noise in the overall trend.
  How could you smooth this further to focus on long-term changes?
\item
  If you look closely (e.g. \texttt{+\ xlim(2008,\ 2012)}) at the
  long-term trend you'll notice a weird pattern in 2009-2011. It looks
  like there was a big dip in 2010. Is this dip ``real''? (i.e.~can you
  spot it in the original data)
\item
  What other variables in the TX housing data show strong seasonal
  effects? Does this technique help to remove them?
\item
  Not all the cities in this data set have complete time series. Use
  your dplyr skills to figure out how much data each city is missing.
  Display the results with a visualisation.
\item
  Replicate the computation that \texttt{stat\_summary()} did with dplyr
  so you can plot the data ``by hand''.
\end{enumerate}

\hypertarget{sub:modelvis}{%
\section{Visualising models}\label{sub:modelvis}}

The previous examples used the linear model just as a tool for removing
trend: we fit the model and immediately threw it away. We didn't care
about the model itself, just what it could do for us. But the models
themselves contain useful information and if we keep them around, there
are many new problems that we can solve:

\begin{itemize}
\item
  We might be interested in cities where the model didn't fit well: a
  poorly fitting model suggests that there isn't much of a seasonal
  pattern, which contradicts our implicit hypothesis that all cities
  share a similar pattern.
\item
  The coefficients themselves might be interesting. In this case,
  looking at the coefficients will show us how the seasonal pattern
  varies between cities.
\item
  We may want to dive into the details of the model itself, and see
  exactly what it says about each observation. For this data, it might
  help us find suspicious data points that might reflect data entry
  errors.
\end{itemize}

To take advantage of this data, we need to store the models. We can do
this using a new dplyr verb: \texttt{do()}. It allows us to store the
result of arbitrary computation in a column. Here we'll use it to store
that linear model: \indexf{do}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models <-}\StringTok{ }\NormalTok{txhousing }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(city) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{do}\NormalTok{(}\DataTypeTok{mod =} \KeywordTok{lm}\NormalTok{(}
    \KeywordTok{log2}\NormalTok{(sales) }\OperatorTok{~}\StringTok{ }\KeywordTok{factor}\NormalTok{(month), }
    \DataTypeTok{data =}\NormalTok{ ., }
    \DataTypeTok{na.action =}\NormalTok{ na.exclude}
\NormalTok{  ))}
\NormalTok{models}
\CommentTok{#> Source: local data frame [46 x 2]}
\CommentTok{#> Groups: <by row>}
\CommentTok{#> }
\CommentTok{#> # A tibble: 46 x 2}
\CommentTok{#>   city      mod     }
\CommentTok{#> * <chr>     <list>  }
\CommentTok{#> 1 Abilene   <S3: lm>}
\CommentTok{#> 2 Amarillo  <S3: lm>}
\CommentTok{#> 3 Arlington <S3: lm>}
\CommentTok{#> 4 Austin    <S3: lm>}
\CommentTok{#> 5 Bay Area  <S3: lm>}
\CommentTok{#> 6 Beaumont  <S3: lm>}
\CommentTok{#> # ... with 40 more rows}
\end{Highlighting}
\end{Shaded}

There are two important things to note in this code:

\begin{itemize}
\item
  \texttt{do()} creates a new column called \texttt{mod.} This is a
  special type of column: instead of containing an atomic vector (a
  logical, integer, numeric, or character) like usual, it's a list.
  Lists are R's most flexible data structure and can hold anything,
  including linear models.
\item
  \texttt{.} is a special pronoun used by \texttt{do()}. It refers to
  the ``current'' data frame. In this example, \texttt{do()} fits the
  model 46 times (once for each city), each time replacing \texttt{.}
  with the data for one city. \indexc{.}
\end{itemize}

If you're an experienced modeller, you might wonder why I didn't fit one
model to all cities simultaneously. That's a great next step, but it's
often useful to start off simple. Once we have a model that works for
each city individually, you can figure out how to generalise it to fit
all cities simultaneously.

To visualise these models, we'll turn them into tidy data frames. We'll
do that with the \textbf{broom} package by David Robinson. \index{broom}
\index{Tidy models} \index{Model data}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

Broom provides three key verbs, each corresponding to one of the
challenges outlined above:

\begin{itemize}
\item
  \texttt{glance()} extracts \textbf{model}-level summaries with one row
  of data for each model. It contains summary statistics like the
  \(R^2\) and degrees of freedom.
\item
  \texttt{tidy()} extracts \textbf{coefficient}-level summaries with one
  row of data for each coefficient in each model. It contains
  information about individual coefficients like their estimate and
  standard error.
\item
  \texttt{augment()} extracts \textbf{observation}-level summaries with
  one row of data for each observation in each model. It includes
  variables like the residual and influence metrics useful for
  diagnosing outliers.
\end{itemize}

We'll learn more about each of these functions in the following three
sections.

\hypertarget{model-level-summaries}{%
\section{Model-level summaries}\label{model-level-summaries}}

We'll begin by looking at how well the model fit to each city with
\texttt{glance()}: \indexf{glance}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model_sum <-}\StringTok{ }\NormalTok{models }\OperatorTok{%>%}\StringTok{ }\KeywordTok{glance}\NormalTok{(mod)}
\NormalTok{model_sum}
\CommentTok{#> # A tibble: 46 x 12}
\CommentTok{#> # Groups: city [46]}
\CommentTok{#>   city   r.sq~ adj.~ sigma stat~   p.value    df logL~   AIC   BIC}
\CommentTok{#>   <chr>  <dbl> <dbl> <dbl> <dbl>     <dbl> <int> <dbl> <dbl> <dbl>}
\CommentTok{#> 1 Abile~ 0.530 0.500 0.282  17.9  1.50e-23    12 -22.2  70.5 112  }
\CommentTok{#> 2 Amari~ 0.449 0.415 0.302  13.0  7.41e-18    12 -35.0  95.9 138  }
\CommentTok{#> 3 Arlin~ 0.513 0.483 0.267  16.8  2.75e-22    12 -12.5  50.9  92.9}
\CommentTok{#> 4 Austin 0.487 0.455 0.310  15.1  2.04e-20    12 -40.3 107   149  }
\CommentTok{#> 5 Bay A~ 0.555 0.527 0.265  19.9  1.45e-25    12 -10.5  47.0  89.0}
\CommentTok{#> 6 Beaum~ 0.430 0.395 0.275  12.0  1.18e-16    12 -18.0  62.1 104  }
\CommentTok{#> # ... with 40 more rows, and 2 more variables: deviance <dbl>,}
\CommentTok{#> #   df.residual <int>}
\end{Highlighting}
\end{Shaded}

This creates a variable with one row for each city, and variables that
either summarise complexity (e.g. \texttt{df}) or fit (e.g.
\texttt{r.squared}, \texttt{p.value}, \texttt{AIC}). Since all the
models we fit have the same complexity (12 terms: one for each month),
we'll focus on the model fit summaries. \(R^2\) is a reasonable place to
start because it's well known. We can use a dot plot to see the
variation across cities:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(model_sum, }\KeywordTok{aes}\NormalTok{(r.squared, }\KeywordTok{reorder}\NormalTok{(city, r.squared))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\linewidth]{_figures/modelling/unnamed-chunk-17-1}
\end{figure}

It's hard to picture exactly what those values of \(R^2\) mean, so it's
helpful to pick out a few exemplars. The following code extracts and
plots out the three cities with the highest and lowest \(R^2\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{top3 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Bryan-College Station"}\NormalTok{, }\StringTok{"Lubbock"}\NormalTok{, }\StringTok{"NE Tarrant County"}\NormalTok{)}
\NormalTok{bottom3 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"McAllen"}\NormalTok{, }\StringTok{"Brownsville"}\NormalTok{, }\StringTok{"Harlingen"}\NormalTok{)}
\NormalTok{extreme <-}\StringTok{ }\NormalTok{txhousing }\OperatorTok{%>%}\StringTok{ }\KeywordTok{ungroup}\NormalTok{() }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(city }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(top3, bottom3), }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(sales)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{city =} \KeywordTok{factor}\NormalTok{(city, }\KeywordTok{c}\NormalTok{(top3, bottom3)))}

\KeywordTok{ggplot}\NormalTok{(extreme, }\KeywordTok{aes}\NormalTok{(month, }\KeywordTok{log}\NormalTok{(sales))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ year)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{facet_wrap}\NormalTok{(}\OperatorTok{~}\NormalTok{city)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\linewidth]{_figures/modelling/unnamed-chunk-18-1}
\end{figure}

The cities with low \(R^2\) have weaker seasonal patterns and more
variation between years. The data for Harlingen seems particularly
noisy.

\hypertarget{exercises-2}{%
\subsection{Exercises}\label{exercises-2}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Do your conclusions change if you use a different measurement of model
  fit like AIC or deviance? Why/why not?
\item
  One possible hypothesis that explains why McAllen, Harlingen and
  Brownsville have lower \(R^2\) is that they're smaller towns so there
  are fewer sales and more noise. Confirm or refute this hypothesis.
\item
  McAllen, Harlingen and Brownsville seem to have much more year-to-year
  variation than Bryan-College Station, Lubbock, and NE Tarrant County.
  How does the model change if you also include a linear trend for year?
  (i.e. \texttt{log(sales)\ \textasciitilde{}\ factor(month)\ +\ year}).
\item
  Create a faceted plot that shows the seasonal patterns for all
  cities.\\
  Order the facets by the \(R^2\) for the city.
\end{enumerate}

\hypertarget{coefficient-level-summaries}{%
\section{Coefficient-level
summaries}\label{coefficient-level-summaries}}

The model fit summaries suggest that there are some important
differences in seasonality between the different cities. Let's dive into
those differences by using \texttt{tidy()} to extract detail about each
individual coefficient: \indexf{tidy}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs <-}\StringTok{ }\NormalTok{models }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tidy}\NormalTok{(mod)}
\NormalTok{coefs}
\CommentTok{#> # A tibble: 552 x 6}
\CommentTok{#> # Groups: city [46]}
\CommentTok{#>   city    term           estimate std.error statistic      p.value}
\CommentTok{#>   <chr>   <chr>             <dbl>     <dbl>     <dbl>        <dbl>}
\CommentTok{#> 1 Abilene (Intercept)       6.54     0.0704     92.9     7.90e-151}
\CommentTok{#> 2 Abilene factor(month)2    0.354    0.0996      3.55    4.91e-  4}
\CommentTok{#> 3 Abilene factor(month)3    0.675    0.0996      6.77    1.83e- 10}
\CommentTok{#> 4 Abilene factor(month)4    0.749    0.0996      7.52    2.76e- 12}
\CommentTok{#> 5 Abilene factor(month)5    0.916    0.0996      9.20    1.06e- 16}
\CommentTok{#> 6 Abilene factor(month)6    1.00     0.0996     10.1     4.37e- 19}
\CommentTok{#> # ... with 546 more rows}
\end{Highlighting}
\end{Shaded}

We're more interested in the month effect, so we'll do a little extra
tidying to only look at the month coefficients, and then to extract the
month value into a numeric variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{months <-}\StringTok{ }\NormalTok{coefs }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{grepl}\NormalTok{(}\StringTok{"factor"}\NormalTok{, term)) }\OperatorTok{%>%}
\StringTok{  }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{extract}\NormalTok{(term, }\StringTok{"month"}\NormalTok{, }\StringTok{"(}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{d+)"}\NormalTok{, }\DataTypeTok{convert =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{months}
\CommentTok{#> # A tibble: 506 x 6}
\CommentTok{#> # Groups: city [46]}
\CommentTok{#>   city    month estimate std.error statistic               p.value}
\CommentTok{#> * <chr>   <int>    <dbl>     <dbl>     <dbl>                 <dbl>}
\CommentTok{#> 1 Abilene     2    0.354    0.0996      3.55              4.91e- 4}
\CommentTok{#> 2 Abilene     3    0.675    0.0996      6.77              1.83e-10}
\CommentTok{#> 3 Abilene     4    0.749    0.0996      7.52              2.76e-12}
\CommentTok{#> 4 Abilene     5    0.916    0.0996      9.20              1.06e-16}
\CommentTok{#> 5 Abilene     6    1.00     0.0996     10.1               4.37e-19}
\CommentTok{#> 6 Abilene     7    0.954    0.0996      9.58              9.81e-18}
\CommentTok{#> # ... with 500 more rows}
\end{Highlighting}
\end{Shaded}

This is a common pattern. You need to use your data tidying skills at
many points in an analysis. Once you have the correct tidy dataset,
creating the plot is usually easy. Here we'll put month on the x-axis,
estimate on the y-axis, and draw one line for each city. I've
back-transformed to make the coefficients more interpretable: these are
now ratios of sales compared to January.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(months, }\KeywordTok{aes}\NormalTok{(month, }\DecValTok{2} \OperatorTok{^}\StringTok{ }\NormalTok{estimate)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{group =}\NormalTok{ city))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\linewidth]{_figures/modelling/unnamed-chunk-21-1}
\end{figure}

The pattern seems similar across the cities. The main difference is the
strength of the seasonal effect. Let's pull that out and plot it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coef_sum <-}\StringTok{ }\NormalTok{months }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(city) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{max =} \KeywordTok{max}\NormalTok{(estimate))}
\KeywordTok{ggplot}\NormalTok{(coef_sum, }\KeywordTok{aes}\NormalTok{(}\DecValTok{2} \OperatorTok{^}\StringTok{ }\NormalTok{max, }\KeywordTok{reorder}\NormalTok{(city, max))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.65\linewidth]{_figures/modelling/unnamed-chunk-22-1}
\end{figure}

The cities with the strongest seasonal effect are College Station and
San Marcos (both college towns) and Galveston and South Padre Island
(beach cities). It makes sense that these cities would have very strong
seasonal effects.

\hypertarget{exercises-3}{%
\subsection{Exercises}\label{exercises-3}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Pull out the three cities with highest and lowest seasonal effect.
  Plot their coefficients.
\item
  How does strength of seasonal effect relate to the \(R^2\) for the
  model? Answer with a plot.
\item
  You should be extra cautious when your results agree with your prior
  beliefs. How can you confirm or refute my hypothesis about the causes
  of strong seasonal patterns?
\item
  Group the diamonds data by cut, clarity and colour. Fit a linear model
  \texttt{log(price)\ \textasciitilde{}\ log(carat)}. What does the
  intercept tell you? What does the slope tell you? How do the slope and
  intercept vary across the groups? Answer with a plot.
\end{enumerate}

\hypertarget{observation-data}{%
\section{Observation data}\label{observation-data}}

Observation-level data, which include residual diagnostics, is most
useful in the traditional model fitting scenario, because it can helps
you find ``high-leverage'' points, point that have a big influence on
the final model. It's also useful in conjunction with visualisation,
particularly because it provides an alternative way to access the
residuals.

Extracting observation-level data is the job of the \texttt{augment()}
function. This adds one row for each observation. It includes the
variables used in the original model, the residuals, and a number of
common influence statistics (see \texttt{?augment.lm} for more details):
\indexf{augment}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs_sum <-}\StringTok{ }\NormalTok{models }\OperatorTok{%>%}\StringTok{ }\KeywordTok{augment}\NormalTok{(mod)}
\NormalTok{obs_sum}
\CommentTok{#> # A tibble: 8,034 x 10}
\CommentTok{#> # Groups: city [46]}
\CommentTok{#>   city   log2.s~ factor.~ .fit~ .se.f~ .resid   .hat .sig~ .cooksd}
\CommentTok{#>   <chr>    <dbl> <fctr>   <dbl>  <dbl>  <dbl>  <dbl> <dbl>   <dbl>}
\CommentTok{#> 1 Abile~    6.17 1         6.54 0.0704 -0.372 0.0625 0.281 0.0103 }
\CommentTok{#> 2 Abile~    6.61 2         6.90 0.0704 -0.281 0.0625 0.282 0.00590}
\CommentTok{#> 3 Abile~    7.02 3         7.22 0.0704 -0.194 0.0625 0.282 0.00282}
\CommentTok{#> 4 Abile~    6.61 4         7.29 0.0704 -0.676 0.0625 0.278 0.0341 }
\CommentTok{#> 5 Abile~    7.14 5         7.46 0.0704 -0.319 0.0625 0.281 0.00760}
\CommentTok{#> 6 Abile~    7.29 6         7.54 0.0704 -0.259 0.0625 0.282 0.00501}
\CommentTok{#> # ... with 8,028 more rows, and 1 more variable: .std.resid <dbl>}
\end{Highlighting}
\end{Shaded}

For example, it might be interesting to look at the distribution of
standardised residuals. (These are residuals standardised to have a
variance of one in each model, making them more comparable). We're
looking for unusual values that might need deeper exploration:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(obs_sum, }\KeywordTok{aes}\NormalTok{(.std.resid)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.1}\NormalTok{)}
\KeywordTok{ggplot}\NormalTok{(obs_sum, }\KeywordTok{aes}\NormalTok{(}\KeywordTok{abs}\NormalTok{(.std.resid))) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_histogram}\NormalTok{(}\DataTypeTok{binwidth =} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]
  \includegraphics[width=0.5\linewidth]{_figures/modelling/unnamed-chunk-25-1}%
  \includegraphics[width=0.5\linewidth]{_figures/modelling/unnamed-chunk-25-2}
\end{figure}

A threshold of 2 seems like a reasonable threshold to explore
individually:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs_sum }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{abs}\NormalTok{(.std.resid) }\OperatorTok{>}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(city) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{(), }\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(}\KeywordTok{abs}\NormalTok{(.std.resid))) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{arrange}\NormalTok{(}\KeywordTok{desc}\NormalTok{(n))}
\CommentTok{#> # A tibble: 43 x 3}
\CommentTok{#>   city                n   avg}
\CommentTok{#>   <chr>           <int> <dbl>}
\CommentTok{#> 1 Texarkana          12  2.43}
\CommentTok{#> 2 Harlingen          11  2.73}
\CommentTok{#> 3 Waco               11  2.96}
\CommentTok{#> 4 Victoria           10  2.49}
\CommentTok{#> 5 Brazoria County     9  2.31}
\CommentTok{#> 6 Brownsville         9  2.48}
\CommentTok{#> # ... with 37 more rows}
\end{Highlighting}
\end{Shaded}

In a real analysis, you'd want to look into these cities in more detail.

\hypertarget{exercises-4}{%
\subsection{Exercises}\label{exercises-4}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A common diagnotic plot is fitted values (\texttt{.fitted})
  vs.~residuals (\texttt{.resid}). Do you see any patterns? What if you
  include the city or month on the same plot?
\item
  Create a time series of log(sales) for each city. Highlight points
  that have a standardised residual of greater than 2.
\end{enumerate}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-model-vis-paper}{}%
Wickham, Hadley, Dianne Cook, and Heike Hofmann. 2015. ``Visualizing
Statistical Models: Removing the Blindfold.'' \emph{Statistical Analysis
and Data Mining: The ASA Data Science Journal} 8 (4):203--25.
